{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d04a6d0",
   "metadata": {},
   "source": [
    "# **Forward Feature Selection Algorithm (FFSA)**\n",
    "\n",
    "### Steps involved in Forward Feature Selection Algorithm:\n",
    "- Start with the empty feature set.\n",
    "- Train ANN model or Random Forest Classifier for each remaining feature.\n",
    "- Estimate the accuracy of adding each feature.\n",
    "- Select the feature that gives the maximum accuracy.\n",
    "- Repeat steps 3-5  and stop when there is no significant improvement.\n",
    "\n",
    "\n",
    "*All the five case studies are being reported for forward feature selection algorithm for obtaining optimal subset of features and maximum accuracy that is possible with that subset of features. The details of each version are as follows:*\n",
    "1. v1 \n",
    "    - The FFSA was run for all 161 statistical features starting with the top ranked feature of random forest as the base feature. The features were appended till there was no significant improvement in       accuracy. Only two features were recorded as the optimal features with accuracy going upto 61.34% with two features being       appended.\n",
    "    - The neural network architecture considered no learning rate.\n",
    "    - Also the code was structured into different separate functions.\n",
    "    \n",
    "2. v2\n",
    "    - The previous code was now improvised and has separate functions for training and for running FFSA on all 161 statistical features starting with an empty set of feature.\n",
    "    - Two changes are been made in ANN architecture:\n",
    "        - lr has been initialised to 0.01. lr=0.1 and lr=0.001 have also been tried but the learning curves performed poorly.\n",
    "        - kernel_initializer='he_uniform' was too initialised.The kernel initializer determines the initial values assigned to the weights, which can significantly impact the learning process and the performance of the model. He initialization is commonly used with activation functions like ReLU (Rectified Linear Unit).It initializes the weights with random values drawn from a Gaussian distribution with zero mean and a standard deviation proportional to the square root of the number of input units feeding into the layer\n",
    "    - This code file focused on **plotting learning curves** for different batch sizes [32,64,128,256,472].This was done to get an understanding of stability and convergence of training. This experiment was performed only for one maximum feature.\n",
    "    - All the plots are available in folder v2_plots.\n",
    "    \n",
    "3. v3\n",
    "    - The FFSA was run for all 161 statistical features starting with the base features that came from intersection of top 20 ranked features of random forest and decision tree algorithm.\n",
    "    - The optimal feature set after running FFSA came out be same as the base features with the maximum accuracy recorded as 69.74%. \n",
    "    - The ANN architecture is same as in v2.\n",
    "    \n",
    "4. v4\n",
    "    - The FFSA was run for all 161 statistical features starting with an empty set of feature.\n",
    "    - First epochs was kept 200 and then 400 to see how the learning curves are going to change and what features are going to be appended.\n",
    "    - The plots are available in folder v4_plots.\n",
    "    \n",
    "5. v5\n",
    "    - The FFSA was run for all 161 statistical features starting with an empty set of features.\n",
    "    - This time Random Forest Classifier was used to train the model to observe change in maximum accuracy and subset of features.\n",
    "    \n",
    " **The presentation for forward feature selection algorithm having all plots and tables is also being added in folder**.\n",
    "\n",
    "6. v6 (same as v5)\n",
    "    - The FFSA was run for all 161 statistical features starting with an empty set of features.\n",
    "    - This time Random Forest Classifier was used to train the model to observe change in maximum accuracy and subset of features.\n",
    "    - There was a mistake in v5 code while shuffling and splitting of data. The correction was made.\n",
    "    - The results are being uploaed in **updated_FFSA presentation file.**\n",
    "\n",
    "7. v7 (same as v4)\n",
    "    - Epochs are being kept as 150.\n",
    "    - There was a mistake in v4 code while shuffling and splitting of data. The correction was made.\n",
    "    - - The results are being uploaed in **updated_FFSA presentation file.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630fd1c2",
   "metadata": {},
   "source": [
    "# Iterative feature Selection Algorithm (IFSA)\n",
    "\n",
    "*All the case studies being reported were just test case studies for developing FFSA. Brief details about each version are as follows:*\n",
    "\n",
    "1. v1\n",
    "    - The common features were selected from intersection of top 20 features that came from random forest and decision tree algorithm.\n",
    "    - The ANN model was trained without any FFSA or IFSA being implemented on those common features.\n",
    "\n",
    "2. v2\n",
    "    - The base features were same as the common features from v1.\n",
    "    - Starting from the base features, the features were added if there was a small increase in accuracy also.\n",
    "    - It appended 13 features in total and accuracy went about 69.74%.\n",
    "    \n",
    "3. v3\n",
    "    - The base features were same as the common features from v1\n",
    "    - The ANN architecture was modified before running IFSA as follows:\n",
    "        - Robust Scaler was chosen instead of Min max scaler.This scaler is suitable when the data contains outliers or has a non-Gaussian distribution. It subtracts the median and divides by the interquartile range (IQR). \n",
    "        - SGD(momentum=0.9) was chosen instead of adam optimizer.\n",
    "        - Batch Gradient Descent was chosen instead of Minibatch gradient descent.\n",
    "    - The comparison table is on slide number 11 for IFSA.\n",
    "    - The selected subset of features were same as those of base features with recorded acuracy of 71.42%.\n",
    "    - The **learning curve** for base feature set too was plooted.\n",
    "     \n",
    "4. v4\n",
    "    - Same ANN architecture as v3 was used.\n",
    "    - In this the base features were the features that were ranked as top 20 features by random forest algorithm.\n",
    "    - IFSA was implemented and only two features were selected using IFSA and accuracy was reported as 61.3%.\n",
    "    \n",
    "**For more details see presentation slides on IFSA.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f66c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
